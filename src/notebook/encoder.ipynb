{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from constant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderSentence(nn.Module):\n",
    "    \n",
    "    def __init__(self,word_size,word_dim, hidden_size, pretrained_word_embeds=None, output_type = 'sum'):\n",
    "        super(EncoderSentence, self).__init__()\n",
    "        \n",
    "        self.output_type = output_type\n",
    "        self.word_size = word_size\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_word_embeds = pretrained_word_embeds\n",
    "        self.embedding = nn.Embedding(self.word_size,self.word_dim,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(self.word_dim,self.hidden_size,batch_first = True,bidirectional=True)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def forward(self,x,input_lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths,batch_first=True)\n",
    "        outputs, hidden_cell = self.lstm(packed)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        #return outputs,hidden_cell\n",
    "        if self.output_type == 'sum':\n",
    "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        elif self.output_type =='concat':\n",
    "            outputs = torch.cat((outputs[:, :, :self.hidden_size], outputs[:, : ,self.hidden_size:]),dim=2)\n",
    "        else:\n",
    "            raise NotImplementedError \n",
    "        return outputs,hidden_cell\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if PRE_TRAINED_EMBEDDING or WORD2VEC_EMBEDDING :\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(self.pretrained_word_embeds))\n",
    "            if NON_TRAINABLE:\n",
    "                self.embedding.weight.requires_grad = False\n",
    "            else:\n",
    "                self.embedding.weight.requires_grad = True\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data_exp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([59, 59, 59, 59, 59, 55, 55, 55, 55, 51, 51, 50, 50, 40, 40, 40, 37, 37,\n",
       "        30, 30, 30, 30, 29, 29, 28, 28, 28, 22, 22, 18, 18, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = EncoderSentence(len(word_mapping)+1,WORD_DIM,128,pretrained_word_embeds,'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e,f = enc(i[1],i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 59, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([59, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         [ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         [ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         ...,\n",
       "         [ 0.0245,  0.0151,  0.0464,  ..., -0.0081,  0.0179, -0.0393],\n",
       "         [ 0.0245,  0.0151,  0.0464,  ..., -0.0081,  0.0179, -0.0393],\n",
       "         [-0.0235, -0.0021,  0.0403,  ..., -0.0370,  0.0243, -0.0374]],\n",
       "\n",
       "        [[-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         [-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         [-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         ...,\n",
       "         [-0.2659, -0.0042,  0.0957,  ...,  0.1353,  0.1373, -0.0414],\n",
       "         [-0.2659, -0.0042,  0.0957,  ...,  0.1353,  0.1373, -0.0414],\n",
       "         [-0.3920,  0.0092,  0.1386,  ...,  0.1341,  0.1699, -0.0537]]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f[0].transpose(0,1).transpose(1,2)).contiguous().view(f[0].transpose(0,1).size()[0],-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0338,  0.0332,  0.0210, -0.0050,  0.0942, -0.0443,  0.0199, -0.0204,\n",
       "         0.0860, -0.0108,  0.0372, -0.0430,  0.0004,  0.0066,  0.0158, -0.0360,\n",
       "        -0.0413, -0.0015, -0.0672, -0.0775, -0.0516,  0.0199, -0.0183, -0.0089,\n",
       "         0.0288,  0.0352,  0.0141, -0.0541, -0.0444,  0.0337,  0.0214, -0.0277,\n",
       "         0.0533,  0.0307,  0.0389, -0.0362,  0.0387, -0.0172,  0.0020, -0.0298,\n",
       "        -0.0013, -0.0397,  0.0080, -0.0435, -0.0059,  0.0558,  0.0454,  0.0708,\n",
       "        -0.1004,  0.0101,  0.0274, -0.0130, -0.0324, -0.0397,  0.0037, -0.0273,\n",
       "        -0.0127,  0.0108,  0.0170,  0.0365, -0.0028, -0.0345, -0.0171,  0.0574,\n",
       "         0.0275,  0.0469, -0.0446,  0.0356, -0.0727,  0.0248, -0.0793,  0.0147,\n",
       "         0.0122, -0.0518, -0.0254,  0.0066, -0.0292, -0.0437,  0.0338, -0.0062,\n",
       "         0.0498,  0.0965,  0.0411, -0.0332,  0.0837,  0.0473,  0.0054,  0.0105,\n",
       "         0.0303, -0.0485,  0.0090, -0.0298,  0.0366, -0.0218, -0.0180,  0.0407,\n",
       "         0.0018, -0.0700,  0.0774, -0.0017,  0.0463,  0.0536, -0.0172, -0.0674,\n",
       "         0.0235, -0.0502,  0.0673, -0.0451,  0.0042,  0.0128,  0.0043, -0.0514,\n",
       "         0.0596, -0.0591, -0.0330,  0.0067,  0.0115, -0.0020,  0.0188,  0.0194,\n",
       "         0.0425, -0.0519, -0.0386,  0.0462, -0.0217, -0.0648,  0.0236, -0.0300],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f[0].view(1,-1,256))[0][0][128:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         [ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         [ 0.0338,  0.0332,  0.0210,  ..., -0.0648,  0.0236, -0.0300],\n",
       "         ...,\n",
       "         [ 0.0245,  0.0151,  0.0464,  ..., -0.0081,  0.0179, -0.0393],\n",
       "         [ 0.0245,  0.0151,  0.0464,  ..., -0.0081,  0.0179, -0.0393],\n",
       "         [-0.0235, -0.0021,  0.0403,  ..., -0.0370,  0.0243, -0.0374]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][0:f[0].size(0):2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         [-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         [-0.4233,  0.0110,  0.1447,  ...,  0.1305,  0.1711, -0.0455],\n",
       "         ...,\n",
       "         [-0.2659, -0.0042,  0.0957,  ...,  0.1353,  0.1373, -0.0414],\n",
       "         [-0.2659, -0.0042,  0.0957,  ...,  0.1353,  0.1373, -0.0414],\n",
       "         [-0.3920,  0.0092,  0.1386,  ...,  0.1341,  0.1699, -0.0537]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][1:f[0].size(0):2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = f[0]\n",
    "k=torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import string\n",
    "import codecs\n",
    "\n",
    "from constant import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filepath):\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(filepath,min_count):\n",
    "    gen = data_generator(filepath)\n",
    "    word_count = {}\n",
    "    word_map = {}\n",
    "    sent_lengths = []\n",
    "    para_lengths = []\n",
    "    for data in tqdm(gen):\n",
    "        ques = data['question'].split()\n",
    "        paragraph = data['paragraph'].split()\n",
    "        sentence = data['sentence'].split()\n",
    "        para_lengths.append(len(paragraph))\n",
    "        sent_lengths.append(len(sentence))\n",
    "        text = ques + paragraph\n",
    "        for item in text:\n",
    "            if word_count.get(item):\n",
    "                word_count[item]+=1\n",
    "            else:\n",
    "                word_count[item] = 1\n",
    "    for k,v in word_count.iteritems():\n",
    "        if v>min_count:\n",
    "            word_map[k] = len(word_map)+1\n",
    "    word_map['<pad>'] = 0\n",
    "    word_map['<unk>'] = len(word_map)\n",
    "    word_map['<start>'] = len(word_map)\n",
    "    word_map['<end>'] = len(word_map)\n",
    "    return word_map,para_lengths,sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f05c60b7420409aab957d67c7fc0c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style=u'info', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_mapping,para_len,sent_len = create_mapping('../data/processed/train_data.json',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum sentence length : 431\n",
      "minimum sentence length : 4\n",
      "mean sentence length : 32.8592020884\n",
      "std dev sentence length : 17.3015877763\n",
      "mean + 3*std_dev sentence length : 84.7639654174\n",
      "\n",
      "\n",
      "maximum paragraph length : 767\n",
      "minimum paragraph length : 22\n",
      "mean paragraph length : 139.626156291\n",
      "std dev paragraph length : 55.4859031596\n",
      "mean + 3*std_dev paragraph length : 306.08386577\n"
     ]
    }
   ],
   "source": [
    "print \"maximum sentence length : {}\".format(max(sent_len))\n",
    "print \"minimum sentence length : {}\".format(min(sent_len))\n",
    "print \"mean sentence length : {}\".format(np.mean(sent_len))\n",
    "print \"std dev sentence length : {}\".format(np.std(sent_len))\n",
    "print \"mean + 3*std_dev sentence length : {}\".format(np.mean(sent_len)+3*np.std(sent_len))\n",
    "print \"\\n\"\n",
    "print \"maximum paragraph length : {}\".format(max(para_len))\n",
    "print \"minimum paragraph length : {}\".format(min(para_len))\n",
    "print \"mean paragraph length : {}\".format(np.mean(para_len))\n",
    "print \"std dev paragraph length : {}\".format(np.std(para_len))\n",
    "print \"mean + 3*std_dev paragraph length : {}\".format(np.mean(para_len)+3*np.std(para_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 90\n",
    "MAX_PARA_LEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48009"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2195875 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "## pre trained embedding\n",
    "#################################################\n",
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(PRE_TRAINED_EMBEDDING_PATH, 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == WORD_DIM + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "pretrained_word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_mapping), WORD_DIM))\n",
    "\n",
    "for w in word_mapping:\n",
    "    if w.lower() in all_word_embeds:\n",
    "        pretrained_word_embeds[word_mapping[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))\n",
    "## To save memory\n",
    "del all_word_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### Dataloader\n",
    "####################################\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self,file_path,length,word2idx):\n",
    "        self.file_path = file_path\n",
    "        self.length = length\n",
    "        self.word2idx = word2idx\n",
    "        self.gen = data_generator(self.file_path)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        try:\n",
    "            text = self.gen.next()\n",
    "        except StopIteration:\n",
    "            self.gen = data_generator(self.file_path)\n",
    "            text = self.gen.next()\n",
    "        paragraph = text['paragraph'].split()\n",
    "        paragraph.insert(0, \"<start>\")\n",
    "        paragraph.append('<end>')\n",
    "        sentence = text['sentence'].split()\n",
    "        sentence.insert(0, \"<start>\")\n",
    "        sentence.append('<end>')\n",
    "        question = text['question'].split()\n",
    "        question.insert(0, \"<start>\")\n",
    "        question.append('<end>')\n",
    "        \n",
    "        x_paragraph = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in paragraph]\n",
    "        x_sentence = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in sentence]\n",
    "        x_question = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in question]\n",
    "        paragraph_word_len = len(paragraph)\n",
    "        sentence_word_len = len(sentence)\n",
    "        question_word_len = len(question)\n",
    "        return {'paragraph': paragraph,'sentence': sentence,'question': question,\n",
    "                'paragraph_word_id':x_paragraph,'sentence_word_id':x_sentence,'question_word_id':x_question,\n",
    "                \"paragraph_word_len\": paragraph_word_len,\"sentence_word_len\": sentence_word_len,\"question_word_len\":question_word_len}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    #paragraph_max_word = [item['paragraph_word_len'] for item in batch]\n",
    "    #sentence_max_word = [item['sentence_word_len'] for item in batch]\n",
    "    question_max_word = [item['question_word_len'] for item in batch]\n",
    "    paragraph_max_len = MAX_PARA_LEN\n",
    "    sentence_max_len = MAX_SENT_LEN\n",
    "    question_max_len = max(question_max_word)\n",
    "\n",
    "    paragraph_word_data = np.zeros((len(batch),paragraph_max_len))\n",
    "    sentence_word_data = np.zeros((len(batch),sentence_max_len))\n",
    "    question_word_data = np.zeros((len(batch),question_max_len))\n",
    "    for i,item in enumerate(batch):\n",
    "        paragraph_word_data[i,:len(item['paragraph_word_id'])] = item['paragraph_word_id']\n",
    "        sentence_word_data[i,:len(item['sentence_word_id'])] = item['sentence_word_id']\n",
    "        question_word_data[i,:len(item['question_word_id'])] = item['question_word_id']\n",
    "    paragraph =[item['paragraph'] for item in batch]\n",
    "    sentence =[item['sentence'] for item in batch]\n",
    "    question =[item['question'] for item in batch]\n",
    "    return torch.tensor(paragraph_word_data),torch.tensor(sentence_word_data),torch.tensor(question_word_data),paragraph,sentence,question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load datas\n",
    "train_dataloader = DataLoader(CustomDataset(TRAIN_DATA_PATH,TRAIN_DATA_LENGTH,word_mapping),\n",
    "                              batch_size=BATCH_SIZE,collate_fn = collate_fn,shuffle=False)\n",
    "\n",
    "valid_dataloader = DataLoader(CustomDataset(VALID_DATA_PATH,VALID_DATA_LENGTH,word_mapping),\n",
    "                              batch_size=BATCH_SIZE,collate_fn = collate_fn,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src1 = batch[0]\n",
    "        src2 = batch[1]\n",
    "        trg = batch[2]\n",
    "        src1 = Variable(src1.type(torch.LongTensor))\n",
    "        src2 = Variable(src2.type(torch.LongTensor))\n",
    "        trg = Variable(trg.type(torch.LongTensor))\n",
    "        if USE_GPU:\n",
    "            src1 = src1.cuda()\n",
    "            src2 = src2.cuda()\n",
    "            trg = trg.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src1,src2, trg)\n",
    "        \n",
    "        #trg = [batch size,sent len]\n",
    "        #output = [batch size,sent len, output dim]\n",
    "        \n",
    "        #reshape to:\n",
    "        #trg = [(sent len - 1) * batch size]\n",
    "        #output = [(sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output[:,1:,:].contiguous().view(-1, output.shape[2]), trg[:,1:].contiguous().view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / iterator.dataset.length,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src1 = batch[0]\n",
    "            src2 = batch[1]\n",
    "            trg = batch[2]\n",
    "            src1 = Variable(src1.type(torch.LongTensor))\n",
    "            src2 = Variable(src2.type(torch.LongTensor))\n",
    "            trg = Variable(trg.type(torch.LongTensor))\n",
    "            if USE_GPU:\n",
    "                src1 = src1.cuda()\n",
    "                src2 = src2.cuda()\n",
    "                trg = trg.cuda()\n",
    "\n",
    "            output = model(src1,src2, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output[:,1:,:].contiguous().view(-1, output.shape[2]), trg[:,1:].contiguous().view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / iterator.dataset.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(filename):\n",
    "    state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(), 'valid_loss': valid_loss}\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SIZE = len(word_mapping)\n",
    "WORD_DIM = 100\n",
    "HIDDEN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48009"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = EncoderParagraph(WORD_SIZE,WORD_DIM,HIDDEN_SIZE,pretrained_word_embeds)\n",
    "enc2 = EncoderSentence(WORD_SIZE,WORD_DIM,HIDDEN_SIZE,pretrained_word_embeds)\n",
    "dec = AttnDecoderLSTM(WORD_SIZE,WORD_DIM,HIDDEN_SIZE*4,MAX_SENT_LEN,pretrained_word_embeds)\n",
    "model = QuestionGeneration(enc1,enc2, dec)\n",
    "\n",
    "if USE_GPU:\n",
    "    enc1 = enc1.cuda()\n",
    "    enc2 = enc2.cuda()\n",
    "    dec = dec.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = word_mapping['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 356])\n",
      "torch.Size([32, 100])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot copy sequence with size 94 to array axis with dimension 90",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-b9ed582475d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-6470435270c7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msrc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shaurya/anaconda2/envs/temp2/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-368ebcd78ab8>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mparagraph_word_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0msentence_word_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mquestion_word_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_word_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mparagraph\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot copy sequence with size 94 to array axis with dimension 90"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "#if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "#    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss,opti = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        save('../models/checkpoint_epoch_'+str(epoch)+'_valid_loss_'+str(valid_loss)+'_'+'.pth.tar')\n",
    "    \n",
    "    print('Epoch [{}/{}] Train Loss: {:.4f} | Val. Loss: {:.4f}'.format(epoch+1, N_EPOCHS, train_loss,valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp2",
   "language": "python",
   "name": "temp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

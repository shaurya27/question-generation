{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from constant import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        last_hidden: (batch_size, hidden_size)\n",
    "        encoder_outputs: (batch_size, max_time, hidden_size)\n",
    "    Returns:\n",
    "        attention_weights: (batch_size, max_time)\n",
    "    \"\"\"\n",
    "    def __init__(self,hidden_size, method=\"general\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if method == 'dot':\n",
    "            pass\n",
    "        elif method == 'general':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size/2, bias=False)\n",
    "            #self.Wa = nn.Linear(hidden_size, (hidden_size/4), bias=False)\n",
    "        elif method == \"concat\":\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        elif method == 'bahdanau':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs, encoder_batch_len):\n",
    "        batch_size, seq_lens, _ = encoder_outputs.size()\n",
    "\n",
    "        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n",
    "        \n",
    "        # masking\n",
    "        maxlen = encoder_outputs.size(1)\n",
    "        mask = torch.arange(maxlen)[None, :] < encoder_batch_len[:, None]\n",
    "        attention_energies[~mask] = float('-inf')\n",
    "        return F.softmax(attention_energies, -1)\n",
    "\n",
    "    def _score(self, last_hidden, encoder_outputs ,method):\n",
    "        \"\"\"\n",
    "        Computes an attention score\n",
    "        :param last_hidden: (batch_size, hidden_dim)\n",
    "        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n",
    "        :param method: str (`dot`, `general`, `concat`, `bahdanau`)\n",
    "        :return: a score (batch_size, max_time)\n",
    "        \"\"\"\n",
    "\n",
    "        #assert encoder_outputs.size()[-1] == self.hidden_size\n",
    "\n",
    "        if method == 'dot':\n",
    "            last_hidden = last_hidden.unsqueeze(-1)\n",
    "            return encoder_outputs.bmm(last_hidden).squeeze(-1)\n",
    "\n",
    "        elif method == 'general':\n",
    "            x = self.Wa(last_hidden)\n",
    "            x = x.unsqueeze(-1)\n",
    "            print x.size(),encoder_outputs.size()\n",
    "            return encoder_outputs.bmm(x).squeeze(-1)\n",
    "\n",
    "        elif method == \"concat\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            x = F.tanh(self.Wa(torch.cat((x, encoder_outputs), 1)))\n",
    "            return x.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        elif method == \"bahdanau\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            out = F.tanh(self.Wa(x) + self.Ua(encoder_outputs))\n",
    "            return out.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data_exp import *\n",
    "from encoder.encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([48, 48, 48, 45, 39, 34, 34, 33, 33, 33, 31, 31, 27, 27, 27, 24, 21, 21,\n",
       "        20, 19, 16, 16, 16, 16, 15, 15, 13, 12, 10,  7,  7,  7])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = EncoderSentence(len(word_mapping)+1,WORD_DIM,128,pretrained_word_embeds,'sum')\n",
    "e,f = enc(i[1],i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 48, 128])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_hidden = torch.rand(32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attn = Attention(128,method='dot')\n",
    "a = attn(last_hidden,e,i[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 48])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0818, 0.1367, 0.1534, 0.1557, 0.1519, 0.1632, 0.1573, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0337, 0.1525, 0.1803, 0.1508, 0.1615, 0.1803, 0.1410, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

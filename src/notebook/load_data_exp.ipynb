{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tqdm \n",
    "import string\n",
    "import codecs\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from constant import *\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WORD2VEC_EMBEDDING:\n",
    "    \n",
    "    def load_vocab(vocab_path):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        word2idx = data\n",
    "        idx2word = dict([(v, k) for k, v in data.items()])\n",
    "        return word2idx, idx2word\n",
    "\n",
    "    word2id,id2word = load_vocab('../../data/word2vec_vocab')\n",
    "    \n",
    "    for i,j in word2id.items():\n",
    "        word2id[i] = j+3\n",
    "\n",
    "    id2word = dict([(v, k) for k, v in word2id.items()])\n",
    "    word2id['<start>'] = 1\n",
    "    id2word[1] = '<start>'\n",
    "    word2id['<end>'] = len(word2id)+1\n",
    "    id2word[len(id2word)+1] = '<end>'\n",
    "    word2id['<unk>'] = len(word2id)+1\n",
    "    id2word[len(id2word)+1] = '<unk>'\n",
    "    \n",
    "    pretrained_word_embeds = np.load(open('../../data/word2vec_weigths', 'rb'))\n",
    "    padded_vector = np.zeros(300)\n",
    "    pretrained_word_embeds = np.insert(pretrained_word_embeds,0,padded_vector,axis=0)\n",
    "    pretrained_word_embeds = np.insert(pretrained_word_embeds,1,np.random.rand(1,300),axis=0)\n",
    "    pretrained_word_embeds = np.insert(pretrained_word_embeds,1,np.random.rand(1,300),axis=0)\n",
    "    pretrained_word_embeds = np.insert(pretrained_word_embeds,1,np.random.rand(1,300),axis=0)\n",
    "    word_mapping = word2id\n",
    "    \n",
    "elif PRE_TRAINED_EMBEDDING:\n",
    "    pretrained_word_embeds = np.load(\"../../data/pre_trained_embedding.npy\")\n",
    "    word_mapping = pickle.load( open( \"../../data/word_mapping.p\", \"rb\" ) )  \n",
    "    \n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MAX_SENT_LEN = 90\n",
    "#MAX_PARA_LEN = 310\n",
    "\n",
    "def data_generator(filepath):\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### Dataloader\n",
    "####################################\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self,file_path,length,word2idx):\n",
    "        self.file_path = file_path\n",
    "        self.length = length\n",
    "        self.word2idx = word2idx\n",
    "        self.gen = data_generator(self.file_path)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        try:\n",
    "            text = self.gen.next()\n",
    "        except StopIteration:\n",
    "            self.gen = data_generator(self.file_path)\n",
    "            text = self.gen.next()\n",
    "        paragraph = text['paragraph'].split()\n",
    "        #paragraph.insert(0, \"<start>\")\n",
    "        #paragraph.append('<end>')\n",
    "        sentence = text['sentence'].split()\n",
    "        #sentence.insert(0, \"<start>\")\n",
    "        #sentence.append('<end>')\n",
    "        question = text['question'].split()\n",
    "        #question.insert(0, \"<start>\")\n",
    "        #question.append('<end>')\n",
    "        \n",
    "        x_paragraph = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in paragraph]\n",
    "        x_sentence = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in sentence]\n",
    "        x_question = [self.word2idx.get(word) if self.word2idx.get(word) else self.word2idx['<unk>'] for word in question]\n",
    "        paragraph_word_len = len(paragraph)\n",
    "        sentence_word_len = len(sentence)\n",
    "        question_word_len = len(question)\n",
    "        return {'paragraph': paragraph,'sentence': sentence,'question': question,\n",
    "                'paragraph_word_id':x_paragraph,'sentence_word_id':x_sentence,'question_word_id':x_question,\n",
    "                \"paragraph_word_len\": paragraph_word_len,\"sentence_word_len\": sentence_word_len,\"question_word_len\":question_word_len}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    \n",
    "def pad(v,max_len = None):\n",
    "    lens = np.array([len(item) for item in v])\n",
    "    if not max_len:\n",
    "        mask = lens[:,None] > np.arange(lens.max())\n",
    "    else:\n",
    "        mask = lens[:,None] > np.arange(max_len)\n",
    "    out = np.zeros(mask.shape,dtype=int)\n",
    "    out[mask] = np.concatenate(v)\n",
    "    return out\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    paragraph_max_word = [item['paragraph_word_len'] for item in batch]\n",
    "    sentence_max_word = [item['sentence_word_len'] for item in batch]\n",
    "    question_max_word = [item['question_word_len'] for item in batch]\n",
    "    \n",
    "    paragraph_max_len = max(paragraph_max_word)\n",
    "    question_max_len = max(question_max_word)\n",
    "    \n",
    "    paragraphs,sentences,questions = [],[],[]\n",
    "    for item in batch:\n",
    "        p = item['paragraph_word_id'][:paragraph_max_len]\n",
    "        p.insert(0, word_mapping[\"<start>\"])\n",
    "        p.append(word_mapping['<end>'])\n",
    "        paragraphs.append(p)\n",
    "        \n",
    "        s = item['sentence_word_id']\n",
    "        s.insert(0, word_mapping[\"<start>\"])\n",
    "        s.append(word_mapping['<end>'])\n",
    "        sentences.append(s)\n",
    "        \n",
    "        q = item['question_word_id'][:question_max_len]\n",
    "        q.insert(0, word_mapping[\"<start>\"])\n",
    "        q.append(word_mapping['<end>'])\n",
    "        questions.append(q)\n",
    "        \n",
    "#     paragraphs = [item['paragraph_word_id'][:paragraph_max_len] for item in batch]\n",
    "#     sentences = [item['sentence_word_id'] for item in batch]\n",
    "#     questions = [item['question_word_id'][:question_max_len] for item in batch]\n",
    "    sort_data = zip(sentences,paragraphs,questions,sentence_max_word)\n",
    "    sort_data.sort(key= lambda x : len(x[0]), reverse=True)\n",
    "    sentences,paragraphs,questions,batch_lengths = zip(*sort_data)\n",
    "    \n",
    "\n",
    "    \n",
    "    sentence_word_data = pad(sentences)\n",
    "    paragraph_word_data = pad(paragraphs,paragraph_max_len+2)\n",
    "    question_word_data = pad(questions,question_max_len+2)\n",
    "\n",
    "    paragraph =[item['paragraph'] for item in batch]\n",
    "    sentence =[item['sentence'] for item in batch]\n",
    "    question =[item['question'] for item in batch]\n",
    "    return torch.tensor(paragraph_word_data),torch.tensor(sentence_word_data),torch.tensor(question_word_data),torch.tensor(batch_lengths),paragraph,sentence,question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load datas\n",
    "train_dataloader = DataLoader(CustomDataset(TRAIN_DATA_PATH,TRAIN_DATA_LENGTH,word_mapping),\n",
    "                              batch_size=BATCH_SIZE,collate_fn = collate_fn,shuffle=False)\n",
    "\n",
    "valid_dataloader = DataLoader(CustomDataset(VALID_DATA_PATH,VALID_DATA_LENGTH,word_mapping),\n",
    "                              batch_size=BATCH_SIZE,collate_fn = collate_fn,shuffle=False)\n",
    "\n",
    "#test_dataloader = DataLoader(CustomDataset(TEST_DATA_PATH,TEST_DATA_LENGTH,word_mapping),\n",
    "#                              batch_size=BATCH_SIZE,collate_fn = collate_fn,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     3,  1897,  ...,  2375,     6, 25055],\n",
       "        [    1,     3,  1897,  ...,  2375,     6, 25055],\n",
       "        [    1,     3,  1897,  ...,  2375,     6, 25055],\n",
       "        ...,\n",
       "        [    1,  1869,     4,  ...,     0,     0,     0],\n",
       "        [    1,  1869,     4,  ...,     0,     0,     0],\n",
       "        [    1,    11,  1880,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " u'a',\n",
       " u'pub',\n",
       " u'/',\n",
       " u'p\\u028cb',\n",
       " u'/',\n",
       " u',',\n",
       " u'or',\n",
       " u'public',\n",
       " u'house',\n",
       " u'is',\n",
       " u',',\n",
       " u'despite',\n",
       " u'its',\n",
       " u'name',\n",
       " u',',\n",
       " u'a',\n",
       " u'private',\n",
       " u'house',\n",
       " u',',\n",
       " u'but',\n",
       " u'is',\n",
       " u'called',\n",
       " u'a',\n",
       " u'public',\n",
       " u'house',\n",
       " u'because',\n",
       " u'it',\n",
       " u'is',\n",
       " u'licensed',\n",
       " u'to',\n",
       " u'sell',\n",
       " u'alcohol',\n",
       " u'to',\n",
       " u'the',\n",
       " u'general',\n",
       " u'public',\n",
       " u'.',\n",
       " u'it',\n",
       " u'is',\n",
       " u'a',\n",
       " u'drinking',\n",
       " u'establishment',\n",
       " u'in',\n",
       " u'britain',\n",
       " u',',\n",
       " u'ireland',\n",
       " u',',\n",
       " u'new',\n",
       " u'zealand',\n",
       " u',',\n",
       " u'australia',\n",
       " u',',\n",
       " u'canada',\n",
       " u',',\n",
       " u'denmark',\n",
       " u'and',\n",
       " u'new',\n",
       " u'england',\n",
       " u'.',\n",
       " u'in',\n",
       " u'many',\n",
       " u'places',\n",
       " u',',\n",
       " u'especially',\n",
       " u'in',\n",
       " u'villages',\n",
       " u',',\n",
       " u'a',\n",
       " u'pub',\n",
       " u'can',\n",
       " u'be',\n",
       " u'the',\n",
       " u'focal',\n",
       " u'point',\n",
       " u'of',\n",
       " u'the',\n",
       " u'community',\n",
       " u'.',\n",
       " u'the',\n",
       " u'writings',\n",
       " u'of',\n",
       " u'samuel',\n",
       " u'pepys',\n",
       " u'describe',\n",
       " u'the',\n",
       " u'pub',\n",
       " u'as',\n",
       " u'the',\n",
       " u'heart',\n",
       " u'of',\n",
       " u'england',\n",
       " u'.',\n",
       " '<end>']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[4][0]l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25055"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['<end>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['<start>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
